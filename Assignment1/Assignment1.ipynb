{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2536, 74)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('Ozone Level Detection.data',\n",
    "                   sep=\";|:|,\",\n",
    "                   header=None,\n",
    "                   engine='python')\n",
    "\n",
    "# change missing value to np.nan\n",
    "data[data == '?'] = np.nan\n",
    "\n",
    "#data.isnull().sum()\n",
    "np_data = data.to_numpy()\n",
    "np_data.shape\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## رفع مشکل داده های گم‌شده\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "برای رفع داده های گم شده از منبع زیر استفاده شده است.\n",
    "https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/\n",
    "\n",
    "\n",
    "Missing values are imputed using the k-Nearest Neighbors approach where a Euclidean distance is used to find the nearest neighbors.\n",
    "\n",
    "من از نزدیک ترین همسایگی استفاده کردم. البته چون داده های مان مربوط به اوزون است و به احتمال زیاد داده های یک روز نزدیک به روز قبل است بنابراین می توان از مقادیر روز قبل و بعد نیز استفاده کرد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "impute_knn = KNNImputer(n_neighbors=5)\n",
    "FixedInputData = impute_knn.fit_transform(X)\n",
    "FixedInputData = pd.DataFrame(FixedInputData)\n",
    "#FixedInputData.insert(0,-1, np_data[:,0]) Adding date as property\n",
    "FixedInputData = FixedInputData.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ورودی این تابع تعداد تقسیم، آرایه و اینکه رندوم بشود یا نه را می گیرد و خروجی آن آرایه ای از کلاس فولد می دهد که شامل داده ی آموزش و تست است. بدیهی است که تعداد این آرایه برابر تعداد تقسیم که در ورودی گرفته است، می باشد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fold:\n",
    "    def __init__(self,X_Train,Y_Train,X_Test,Y_Test):\n",
    "        self.X_Train = X_Train\n",
    "        self.Y_Train = Y_Train\n",
    "        self.X_Test = X_Test\n",
    "        self.Y_Test = Y_Test\n",
    "\n",
    "\n",
    "def KFold(NumberOfSplits, InputData, Shuffle=False):\n",
    "    if(Shuffle):\n",
    "         np.random.shuffle(InputData)\n",
    "    output = []\n",
    "\n",
    "    SplitedArray = np.array_split(InputData, NumberOfSplits)\n",
    "    for i in range(NumberOfSplits):\n",
    "        test = SplitedArray[i]\n",
    "        train = np.concatenate( SplitedArray[0:i] +  SplitedArray[i + 1 :], axis=0 )\n",
    "        X_Train = preprocessing.normalize(train[:,:-1])\n",
    "        Y_Train = train[:, -1]\n",
    "\n",
    "        X_Test = preprocessing.normalize(test[:,:-1])\n",
    "        Y_Test = test[:,-1]\n",
    "        output.append(Fold(X_Train,Y_Train,X_Test,Y_Test))\n",
    "    \n",
    "    return output\n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_Out = KFold(3,FixedInputData)\n",
    "kfold_Out[0].X_Train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## پیادهسازی مدل رگرسیون لاجیستیک"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1 +  np.e ** (-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLossFunction(M, Y_Hat, Y, Regularization,W):\n",
    "    return (-1/M) * (Y.dot( np.log(Y_Hat).T) + (1-Y).dot( np.log(1 - Y_Hat).T)) + Regularization/2*np.sum(np.power(W, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateYHat(W,X,b):\n",
    "    return np.dot(W , np.transpose(X)) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(W,b,X,Y):\n",
    "    Z = CalculateYHat(W,X,b)\n",
    "    A = sigmoid(Z)\n",
    "    Y_hat = np.round(A)\n",
    "    return (Y_hat == Y).sum() / Y.shape[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndTest(NumberOfFolds, LearningRate, NumberOfEpoch, Regularization):\n",
    "    \n",
    "    nr_split = NumberOfFolds\n",
    "    kfold_Out = KFold(nr_split,FixedInputData,True)\n",
    "\n",
    "\n",
    "    learningRate = LearningRate\n",
    "    nr_epoch = NumberOfEpoch\n",
    "    regularization = Regularization\n",
    "\n",
    "    TestLossArray = np.zeros(nr_split)\n",
    "    TestAccuracyArray = np.zeros(nr_split)\n",
    "    TrainAccuracyArray = np.zeros(nr_split)\n",
    "    \n",
    "    for i in range (nr_split): #TODO\n",
    "        # for each split do the training\n",
    "        X = kfold_Out[i].X_Train\n",
    "        Y = np.reshape( kfold_Out[i].Y_Train,(1, kfold_Out[i].Y_Train.shape[0]))\n",
    "        #print(\"shape: \",X.shape)\n",
    "        M = X.shape[0]\n",
    "        W = np.zeros((1 ,X.shape[1])) # 1 * 72\n",
    "        b = np.zeros((1,1)) # 1 * 1\n",
    "\n",
    "        for j in range(nr_epoch): # traning for nr_epoch\n",
    "                \n",
    "            Z = CalculateYHat(W,X,b)  # W => 1 * 72 ||||| X => M * 72 \n",
    "            \n",
    "            A = sigmoid(Z) # Y_hat\n",
    "            #print(A)\n",
    "            \n",
    "            \n",
    "            # dL/dZ\n",
    "            dZ = A - Y \n",
    "            \n",
    "            \n",
    "            # Calculate derivitives\n",
    "            dW = (1/M) * (np.dot(X.T,dZ.T)) + regularization * (W.T) # dL/dW\n",
    "            db = (1/M) * np.sum(dZ) # dL/db\n",
    "\n",
    "            # update the weights and bias\n",
    "            W = W - learningRate * np.transpose(dW)\n",
    "            \n",
    "            b = b - learningRate * np.transpose(db)\n",
    "\n",
    "        # Calculate loss for current split\n",
    "        X_test = kfold_Out[i].X_Test\n",
    "        Y_test = np.reshape( kfold_Out[i].Y_Test,(1, kfold_Out[i].Y_Test.shape[0]))\n",
    "        Z = CalculateYHat(W,X_test,b)\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "        Y_hat = np.round(A)\n",
    "        TestLossArray[i] = CrossEntropyLossFunction(M,A, Y_test,Regularization, W)\n",
    "        TestAccuracyArray[i] =(Y_hat == Y_test).sum() / Y_test.shape[1]\n",
    "\n",
    "\n",
    "        TrainAccuracyArray[i] = Accuracy(W,b,X,Y)\n",
    "        #print((Y_hat == Y_test).sum() / Y_test.shape[1])\n",
    "\n",
    "    print (\"Average Of Train Accuracy: \", np.mean(TrainAccuracyArray))\n",
    "    print (\"Average Of Test Accuracy: \", np.mean(TestAccuracyArray))\n",
    "    print(\"average of Loss: \", np.mean(TestLossArray))\n",
    "    print(\"standard deviation of Loss: \", np.std(TestLossArray))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ارزیابی مدل رگرسیون\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Regularization:  0.1\n",
      "Average Of Train Accuracy:  0.9712149364835532\n",
      "Average Of Test Accuracy:  0.9712162118054097\n",
      "average of Loss:  0.26202669220131364\n",
      "standard deviation of Loss:  0.0006876800959503983\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.01    Number of Epochs:  300    Regularization:  0.01\n",
      "Average Of Train Accuracy:  0.9712142366420672\n",
      "Average Of Test Accuracy:  0.9712134140939378\n",
      "average of Loss:  0.12106968693258004\n",
      "standard deviation of Loss:  0.0019722958028713525\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.0001    Number of Epochs:  500    Regularization:  0.6\n",
      "Average Of Train Accuracy:  0.9712155196847915\n",
      "Average Of Test Accuracy:  0.9712185432316365\n",
      "average of Loss:  0.3359824592045387\n",
      "standard deviation of Loss:  0.00037001671946424125\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Regularization:  1\n",
      "Average Of Train Accuracy:  0.9712155196847915\n",
      "Average Of Test Accuracy:  0.9712185432316365\n",
      "average of Loss:  0.2804941386013906\n",
      "standard deviation of Loss:  0.0012562279061475985\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Regularization:  10\n",
      "Average Of Train Accuracy:  0.9712147032030579\n",
      "Average Of Test Accuracy:  0.9712152792349191\n",
      "average of Loss:  0.298349561069627\n",
      "standard deviation of Loss:  0.00042948895465896023\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Regularization:  0\n",
      "Average Of Train Accuracy:  0.9712151697640484\n",
      "Average Of Test Accuracy:  0.9712171443759005\n",
      "average of Loss:  0.2590505622002563\n",
      "standard deviation of Loss:  0.0008733792995567108\n",
      "\n",
      "\n",
      "Number of folds:  6   Learning rate:  0.001    Number of Epochs:  500    Regularization:  0\n",
      "Average Of Train Accuracy:  0.9712148205392487\n",
      "Average Of Test Accuracy:  0.9712222558345377\n",
      "average of Loss:  0.1036221264879244\n",
      "standard deviation of Loss:  0.0008768908891444878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Test_Vars = [[3,0.001, 500, 0.1],\\\n",
    "             [3,0.01, 300, 0.01],\\\n",
    "             [3,0.0001, 500, 0.6],\\\n",
    "             [3,0.001, 500, 1],\\\n",
    "             [3,0.001, 500, 10],\\\n",
    "             [3,0.001, 500, 0],\\\n",
    "             [6,0.001, 500, 0]]\n",
    "\n",
    "for i in range(len(Test_Vars)):\n",
    "    print()\n",
    "    print(\"Number of folds: \",Test_Vars[i][0],\"  Learning rate: \", Test_Vars[i][1],\"   Number of Epochs: \", Test_Vars[i][2], \"   Regularization: \",Test_Vars[i][3]  )\n",
    "    TrainAndTest(Test_Vars[i][0], Test_Vars[i][1], Test_Vars[i][2], Test_Vars[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "همانطور که در بالا قابل مشاهده است، هایپرپارامتر های من تعداد فولد ها، ضریب یادگیری، تعداد ایپاک ها، و ضریب رگولاریزیشن است. متاسفانه به دلیل اینکه داده ها مناسب نمی باشند نمیتوان نتیجه ی متمایز کننده ای از تغییر این متغیر ها بدست آورد.\n",
    "البته میانگین لاس برای حالت آخر از همه بهتر بوده\n",
    "\n",
    "اورفیت زمانی اتفاق می افتد که دقت داده ترین بالا باشد ولی دقت داده تست پایین باشد و آندر فیت زمانی اتفاق می افتد که مدل ما ساده باشد و بنابراین دقت داده های ترین پایین باشد. در اینجا میبینیم که دقت داده های تست و ترین نزدیک به هم و بالا هستند بنابراین مدل ما نه آندر فیت است و نه اورفیت"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural  Netwrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "یکی از ورودی های کلاس فید فورواردمون آرایه ای است که تعداد آن نشان دهنده ی تعداد لایه ها و مقدار قرار گرفته در هر لایه نشان دهنده تعداد نورون ها در آن لایه است"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, X, layers):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.num_layers = len(layers)\n",
    "            num_feature = X.shape[1]\n",
    "            self.fcs = nn.ModuleList()\n",
    "\n",
    "            if(self.num_layers == 0):\n",
    "                self.fcs.append(nn.Linear(num_feature,1)) # connect input to output   \n",
    "            else:\n",
    "                #First layer\n",
    "                self.fcs.append(nn.Linear(num_feature,layers[0]))\n",
    "\n",
    "                #Loop through all other layers except the last one\n",
    "                #print(self.num_layers)\n",
    "                for i in range(1,self.num_layers):\n",
    "                    self.fcs.append(nn.Linear(layers[i-1],layers[i]))\n",
    "\n",
    "                #Last layer\n",
    "                \n",
    "                self.fcs.append(nn.Linear(layers[-1],1))\n",
    "            \n",
    "        def forward(self, x):\n",
    "            if(self.num_layers == 0):\n",
    "                return torch.sigmoid(self.fcs[-1](x))\n",
    "            else:\n",
    "\n",
    "                # first layer\n",
    "                hidden = self.fcs[0](x)\n",
    "                relu_ = F.relu(hidden)\n",
    "\n",
    "                # every other layers except the last one\n",
    "                for i in range(1, self.num_layers):\n",
    "                    \n",
    "                    hidden = self.fcs[i](relu_)\n",
    "                    relu_ = F.relu(hidden)\n",
    "\n",
    "                # last layer usess sigmoid\n",
    "                output = self.fcs[-1](relu_)\n",
    "                output = torch.sigmoid(output)\n",
    "                return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NueralTrainAndTest(NumberOfFolds, LearningRate, NumberOfEpoch,Layers):\n",
    "    nr_split = NumberOfFolds\n",
    "    kfold_Out = KFold(nr_split,FixedInputData,True)\n",
    "\n",
    "    nr_epoch = NumberOfEpoch\n",
    "    layers = Layers\n",
    "\n",
    "    TestLossArray = np.zeros(nr_split)\n",
    "    TestAccuracyArray = np.zeros(nr_split)\n",
    "    TrainAccuracyArray = np.zeros(nr_split)\n",
    "\n",
    "\n",
    "    for i in range (nr_split): #TODO\n",
    "        # for each split do the training\n",
    "        X = torch.from_numpy(kfold_Out[i].X_Train).float()\n",
    "        Y = torch.from_numpy( kfold_Out[i].Y_Train).float()\n",
    "        M = X.shape[0]\n",
    "\n",
    "        model = Feedforward(X,layers)\n",
    "        criterion = torch.nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = LearningRate)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(nr_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(X)\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), Y)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        X_test = torch.from_numpy(kfold_Out[i].X_Test).float()\n",
    "        Y_test = torch.from_numpy(kfold_Out[i].Y_Test).float()\n",
    "        model.eval()\n",
    "        y_pred = model(X_test)\n",
    "        after_train = criterion(y_pred.squeeze(), Y_test) \n",
    "        Y_test = Y_test.detach().numpy()\n",
    "        y_pred = np.round(y_pred.detach().numpy()[:,0])\n",
    "        #print((y_pred == Y_test).sum() / Y_test.shape[0])\n",
    "\n",
    "        TestLossArray[i] = after_train.item()\n",
    "        TestAccuracyArray[i] = (y_pred == Y_test).sum() / Y_test.shape[0]\n",
    "\n",
    "\n",
    "        #TrainAccuracyArray[i] = Accuracy(W,b,X,Y)\n",
    "        \n",
    "    #print (\"Average Of Train Accuracy: \", np.mean(TrainAccuracyArray))\n",
    "    print (\"Average Of Test Accuracy: \", np.mean(TestAccuracyArray))\n",
    "    print(\"average of Loss: \", np.mean(TestLossArray))\n",
    "    print(\"standard deviation of Loss: \", np.std(TestLossArray))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ارزیابی مدل شبکه عصبی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of folds:  3   Learning rate:  0.01    Number of Epochs:  500    Layers:  [2, 3]\n",
      "Average Of Test Accuracy:  0.9712138803791831\n",
      "average of Loss:  0.19726609190305075\n",
      "standard deviation of Loss:  0.06572045533373759\n",
      "\n",
      "\n",
      "Number of folds:  4   Learning rate:  0.01    Number of Epochs:  500    Layers:  [2, 3]\n",
      "Average Of Test Accuracy:  0.9712145110410095\n",
      "average of Loss:  0.1499730609357357\n",
      "standard deviation of Loss:  0.005558535432413567\n",
      "\n",
      "\n",
      "Number of folds:  5   Learning rate:  0.01    Number of Epochs:  500    Layers:  [2, 3]\n",
      "Average Of Test Accuracy:  0.971215580300983\n",
      "average of Loss:  0.2376216620206833\n",
      "standard deviation of Loss:  0.04527516095910024\n",
      "\n",
      "\n",
      "Number of folds:  6   Learning rate:  0.01    Number of Epochs:  500    Layers:  [2, 3]\n",
      "Average Of Test Accuracy:  0.9712147864310815\n",
      "average of Loss:  0.19616582865516344\n",
      "standard deviation of Loss:  0.055179416928565025\n",
      "\n",
      "\n",
      "Number of folds:  7   Learning rate:  0.01    Number of Epochs:  500    Layers:  [2, 3]\n",
      "Average Of Test Accuracy:  0.9712113602118625\n",
      "average of Loss:  0.19111640538488114\n",
      "standard deviation of Loss:  0.03979214245119425\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Test_Vars = [[3,0.01, 500, [2,3]],\\\n",
    "             [4,0.01, 500, [2,3]],\\\n",
    "             [5,0.01, 500, [2,3]],\\\n",
    "             [6,0.01, 500, [2,3]],\\\n",
    "             [7,0.01, 500, [2,3]]]\n",
    "\n",
    "for i in range(len(Test_Vars)):\n",
    "    print()\n",
    "    print(\"Number of folds: \",Test_Vars[i][0],\"  Learning rate: \", Test_Vars[i][1],\"   Number of Epochs: \", Test_Vars[i][2], \"   Layers: \",Test_Vars[i][3]  )\n",
    "    NueralTrainAndTest(Test_Vars[i][0], Test_Vars[i][1], Test_Vars[i][2], Test_Vars[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "آزمون بالا با تعداد فولد های متفاوت اتفاق افتاده است. متاسفانه به دلیل خوب نبودن داده ها تمایز خاصی دیده نمیشود. فقط در حالت دوم میانگین لاس از همه کمتر بوده است"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Layers:  []\n",
      "Average Of Test Accuracy:  0.9712143466644285\n",
      "average of Loss:  0.5061007738113403\n",
      "standard deviation of Loss:  0.016959712085407554\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.01    Number of Epochs:  300    Layers:  [2, 3]\n",
      "Average Of Test Accuracy:  0.9712143466644285\n",
      "average of Loss:  0.3275444010893504\n",
      "standard deviation of Loss:  0.06732790700040253\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Layers:  [10, 20, 33]\n",
      "Average Of Test Accuracy:  0.9712148129496739\n",
      "average of Loss:  0.5772304137547811\n",
      "standard deviation of Loss:  0.03435666092191804\n",
      "\n",
      "\n",
      "Number of folds:  3   Learning rate:  0.001    Number of Epochs:  500    Layers:  [120, 3, 20]\n",
      "Average Of Test Accuracy:  0.9712138803791831\n",
      "average of Loss:  0.4645797610282898\n",
      "standard deviation of Loss:  0.004656840521461322\n",
      "\n",
      "\n",
      "Number of folds:  6   Learning rate:  0.001    Number of Epochs:  500    Layers:  [1]\n",
      "Average Of Test Accuracy:  0.9712101180539215\n",
      "average of Loss:  0.5009082307418188\n",
      "standard deviation of Loss:  0.1597401247536694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Test_Vars = [[3,0.001, 500, []],\\\n",
    "             [3,0.01, 300, [2,3]],\\\n",
    "             [3,0.001, 500, [10,20,33]],\\\n",
    "             [3,0.001, 500,[120,3,20]],\\\n",
    "             [6,0.001, 500, [1]]]\n",
    "\n",
    "for i in range(len(Test_Vars)):\n",
    "    print()\n",
    "    print(\"Number of folds: \",Test_Vars[i][0],\"  Learning rate: \", Test_Vars[i][1],\"   Number of Epochs: \", Test_Vars[i][2], \"   Layers: \",Test_Vars[i][3]  )\n",
    "    NueralTrainAndTest(Test_Vars[i][0], Test_Vars[i][1], Test_Vars[i][2], Test_Vars[i][3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در اینجا نیز همانند قسمت لاجیستیک تغییر هایپر پارامتر ها ما را به نتیجه ی متمایزی نمیرساند. هایپر پارامتر های ما در این تست تعداد فولد، ضریب یادگیری، تعداد ایپاک ها و تعداد لایه و نورون موجود در هر لایه است.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "البته حالت آخر نتیجه ی خوبی را ارایه نداده ولی حالت های دیگر همگی نتیجه ی خوبی داشتند. مقدار لاس در حالت دوم از همه کمتر بود"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "دوباره در اینجا نیز از آنجایی که نتیجه ی داده تست و داده ترین نزدیک هم هستند و هردو دقت بالایی دارند بنابراین نه اورفیت اتفاق افتاده و نه آندر فیت"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9160b474c77210d4dea9e1e6d815bf0fb870eaba2dea6b3b77c81c1cd918dc32"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
