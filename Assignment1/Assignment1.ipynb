{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('Ozone Level Detection.data',\n",
    "                   sep=\";|:|,\",\n",
    "                   header=None,\n",
    "                   engine='python')\n",
    "\n",
    "# change missing value to np.nan\n",
    "data[data == '?'] = np.nan\n",
    "\n",
    "#data.isnull().sum()\n",
    "np_data = data.to_numpy()\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#رفع داده های گم‌شده\n",
    "\n",
    "برای رفع داده های گم شده از منبع زیر استفاده شده است.\n",
    "https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/\n",
    "\n",
    "\n",
    "Missing values are imputed using the k-Nearest Neighbors approach where a Euclidean distance is used to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,1:74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "impute_knn = KNNImputer(n_neighbors=5)\n",
    "FixedInputData = impute_knn.fit_transform(X)\n",
    "FixedInputData = pd.DataFrame(FixedInputData)\n",
    "#FixedInputData.insert(0,-1, np_data[:,0]) Adding date as property\n",
    "FixedInputData = FixedInputData.to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementing KFold\n",
    "\n",
    "ورودی این تابع تعداد تقسیم، آرایه و اینکه رندوم بشود یا نه را می گیرد و خروجی آن آرایه ای از کلاس فولد می دهد که شامل داده ی آموزش و تست است. بدیهی است که تعداد این آرایه برابر تعداد تقسیم که در ورودی گرفته است، می باشد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fold:\n",
    "    def __init__(self,X_Train,Y_Train,X_Test,Y_Test):\n",
    "        self.X_Train = X_Train\n",
    "        self.Y_Train = Y_Train\n",
    "        self.X_Test = X_Test\n",
    "        self.Y_Test = Y_Test\n",
    "\n",
    "\n",
    "def KFold(NumberOfSplits, InputData, Shuffle=False):\n",
    "    if(Shuffle):\n",
    "         np.random.shuffle(InputData)\n",
    "    output = []\n",
    "\n",
    "    SplitedArray = np.array_split(InputData, NumberOfSplits)\n",
    "    for i in range(NumberOfSplits):\n",
    "        test = SplitedArray[i]\n",
    "        train = np.concatenate( SplitedArray[0:i] +  SplitedArray[i + 1 :], axis=0 )\n",
    "        X_Train = train[:,:-1]\n",
    "        Y_Train = train[:, -1]\n",
    "\n",
    "        X_Test = test[:,:-1]\n",
    "        Y_Test = test[:,-1]\n",
    "        output.append(Fold(X_Train,Y_Train,X_Test,Y_Test))\n",
    "    \n",
    "    return output\n",
    " \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfold_Out = KFold(3,FixedInputData)\n",
    "kfold_Out[0].X_Train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (1/(1 +  np.e ** (-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLossFunction(M, Y_Hat, Y):\n",
    "    return (-1/M) * (Y.dot( np.log(Y_Hat).T) + (1-Y).dot( np.log(1 - Y_Hat).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateYHat(W,X,b):\n",
    "    return np.dot(W , np.transpose(X)) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0  loss: [[0.69314718]]\n",
      "i: 1  loss: [[1.76838483]]\n",
      "i: 2  loss: [[1.67570469]]\n",
      "i: 3  loss: [[1.58302455]]\n",
      "i: 4  loss: [[1.49034441]]\n",
      "i: 5  loss: [[1.39766427]]\n",
      "i: 6  loss: [[1.30498413]]\n",
      "i: 7  loss: [[1.21230399]]\n",
      "i: 8  loss: [[1.11962385]]\n",
      "i: 9  loss: [[1.02694371]]\n",
      "i: 10  loss: [[0.93426357]]\n",
      "i: 11  loss: [[0.84158343]]\n",
      "i: 12  loss: [[0.74890329]]\n",
      "i: 13  loss: [[0.65622315]]\n",
      "i: 14  loss: [[0.56354301]]\n",
      "i: 15  loss: [[0.47086288]]\n",
      "i: 16  loss: [[0.37818301]]\n",
      "i: 17  loss: [[0.28551403]]\n",
      "i: 18  loss: [[0.19329432]]\n",
      "i: 19  loss: [[0.11838901]]\n",
      "i: 20  loss: [[0.12778445]]\n",
      "i: 21  loss: [[0.18592147]]\n",
      "i: 22  loss: [[0.1167162]]\n",
      "i: 23  loss: [[0.11807003]]\n",
      "i: 24  loss: [[0.12755242]]\n",
      "i: 25  loss: [[0.15278211]]\n",
      "i: 26  loss: [[0.30384197]]\n",
      "i: 27  loss: [[0.21138231]]\n",
      "i: 28  loss: [[0.12751934]]\n",
      "i: 29  loss: [[0.15275217]]\n",
      "i: 30  loss: [[0.30375206]]\n",
      "i: 31  loss: [[0.2112931]]\n",
      "i: 32  loss: [[0.12745697]]\n",
      "i: 33  loss: [[0.15269636]]\n",
      "i: 34  loss: [[0.30355145]]\n",
      "i: 35  loss: [[0.21109417]]\n",
      "i: 36  loss: [[0.12732318]]\n",
      "i: 37  loss: [[0.15257487]]\n",
      "i: 38  loss: [[0.30306958]]\n",
      "i: 39  loss: [[0.21061655]]\n",
      "i: 40  loss: [[0.12700961]]\n",
      "i: 41  loss: [[0.15227192]]\n",
      "i: 42  loss: [[0.30180965]]\n",
      "i: 43  loss: [[0.20936828]]\n",
      "i: 44  loss: [[0.12621105]]\n",
      "i: 45  loss: [[0.15136509]]\n",
      "i: 46  loss: [[0.29794974]]\n",
      "i: 47  loss: [[0.20554824]]\n",
      "i: 48  loss: [[0.12391603]]\n",
      "i: 49  loss: [[0.14751476]]\n",
      "i: 50  loss: [[0.2812072]]\n",
      "i: 51  loss: [[0.18907218]]\n",
      "i: 52  loss: [[0.11717867]]\n",
      "i: 53  loss: [[0.1214333]]\n",
      "i: 54  loss: [[0.14981269]]\n",
      "i: 55  loss: [[0.13644525]]\n",
      "i: 56  loss: [[0.23038571]]\n",
      "i: 57  loss: [[0.14185666]]\n",
      "i: 58  loss: [[0.14609272]]\n",
      "i: 59  loss: [[0.27499628]]\n",
      "i: 60  loss: [[0.18301463]]\n",
      "i: 61  loss: [[0.11638375]]\n",
      "i: 62  loss: [[0.11648708]]\n",
      "i: 63  loss: [[0.11722929]]\n",
      "i: 64  loss: [[0.12189535]]\n",
      "i: 65  loss: [[0.1528086]]\n",
      "i: 66  loss: [[0.13287592]]\n",
      "i: 67  loss: [[0.21296289]]\n",
      "i: 68  loss: [[0.12853281]]\n",
      "i: 69  loss: [[0.15343684]]\n",
      "i: 70  loss: [[0.30705218]]\n",
      "i: 71  loss: [[0.21456488]]\n",
      "i: 72  loss: [[0.12962836]]\n",
      "i: 73  loss: [[0.15388109]]\n",
      "i: 74  loss: [[0.30897937]]\n",
      "i: 75  loss: [[0.21647763]]\n",
      "i: 76  loss: [[0.13097678]]\n",
      "i: 77  loss: [[0.1540397]]\n",
      "i: 78  loss: [[0.30969012]]\n",
      "i: 79  loss: [[0.21718329]]\n",
      "i: 80  loss: [[0.1314826]]\n",
      "i: 81  loss: [[0.15399223]]\n",
      "i: 82  loss: [[0.30952561]]\n",
      "i: 83  loss: [[0.21701984]]\n",
      "i: 84  loss: [[0.13136107]]\n",
      "i: 85  loss: [[0.15399319]]\n",
      "i: 86  loss: [[0.30956666]]\n",
      "i: 87  loss: [[0.21706053]]\n",
      "i: 88  loss: [[0.13138738]]\n",
      "i: 89  loss: [[0.1539782]]\n",
      "i: 90  loss: [[0.30953994]]\n",
      "i: 91  loss: [[0.21703391]]\n",
      "i: 92  loss: [[0.13136498]]\n",
      "i: 93  loss: [[0.15396826]]\n",
      "i: 94  loss: [[0.30953461]]\n",
      "i: 95  loss: [[0.21702854]]\n",
      "i: 96  loss: [[0.13135796]]\n",
      "i: 97  loss: [[0.15395672]]\n",
      "i: 98  loss: [[0.30952241]]\n",
      "i: 99  loss: [[0.21701635]]\n",
      "i: 100  loss: [[0.13134601]]\n",
      "i: 101  loss: [[0.15394571]]\n",
      "i: 102  loss: [[0.30951241]]\n",
      "i: 103  loss: [[0.21700634]]\n",
      "i: 104  loss: [[0.13133564]]\n",
      "i: 105  loss: [[0.15393455]]\n",
      "i: 106  loss: [[0.30950171]]\n",
      "i: 107  loss: [[0.21699563]]\n",
      "i: 108  loss: [[0.13132477]]\n",
      "i: 109  loss: [[0.15392345]]\n",
      "i: 110  loss: [[0.30949123]]\n",
      "i: 111  loss: [[0.21698515]]\n",
      "i: 112  loss: [[0.13131407]]\n",
      "i: 113  loss: [[0.15391236]]\n",
      "i: 114  loss: [[0.30948069]]\n",
      "i: 115  loss: [[0.21697459]]\n",
      "i: 116  loss: [[0.13130332]]\n",
      "i: 117  loss: [[0.15390128]]\n",
      "i: 118  loss: [[0.30947016]]\n",
      "i: 119  loss: [[0.21696406]]\n",
      "i: 120  loss: [[0.1312926]]\n",
      "i: 121  loss: [[0.15389022]]\n",
      "i: 122  loss: [[0.30945963]]\n",
      "i: 123  loss: [[0.21695353]]\n",
      "i: 124  loss: [[0.13128187]]\n",
      "i: 125  loss: [[0.15387916]]\n",
      "i: 126  loss: [[0.30944911]]\n",
      "i: 127  loss: [[0.216943]]\n",
      "i: 128  loss: [[0.13127116]]\n",
      "i: 129  loss: [[0.15386813]]\n",
      "i: 130  loss: [[0.30943859]]\n",
      "i: 131  loss: [[0.21693247]]\n",
      "i: 132  loss: [[0.13126044]]\n",
      "i: 133  loss: [[0.15385711]]\n",
      "i: 134  loss: [[0.30942806]]\n",
      "i: 135  loss: [[0.21692194]]\n",
      "i: 136  loss: [[0.13124974]]\n",
      "i: 137  loss: [[0.1538461]]\n",
      "i: 138  loss: [[0.30941754]]\n",
      "i: 139  loss: [[0.21691142]]\n",
      "i: 140  loss: [[0.13123903]]\n",
      "i: 141  loss: [[0.1538351]]\n",
      "i: 142  loss: [[0.30940702]]\n",
      "i: 143  loss: [[0.21690089]]\n",
      "i: 144  loss: [[0.13122834]]\n",
      "i: 145  loss: [[0.15382412]]\n",
      "i: 146  loss: [[0.3093965]]\n",
      "i: 147  loss: [[0.21689037]]\n",
      "i: 148  loss: [[0.13121765]]\n",
      "i: 149  loss: [[0.15381315]]\n",
      "i: 150  loss: [[0.30938599]]\n",
      "i: 151  loss: [[0.21687985]]\n",
      "i: 152  loss: [[0.13120696]]\n",
      "i: 153  loss: [[0.1538022]]\n",
      "i: 154  loss: [[0.30937547]]\n",
      "i: 155  loss: [[0.21686933]]\n",
      "i: 156  loss: [[0.13119628]]\n",
      "i: 157  loss: [[0.15379126]]\n",
      "i: 158  loss: [[0.30936496]]\n",
      "i: 159  loss: [[0.21685881]]\n",
      "i: 160  loss: [[0.13118561]]\n",
      "i: 161  loss: [[0.15378033]]\n",
      "i: 162  loss: [[0.30935445]]\n",
      "i: 163  loss: [[0.21684829]]\n",
      "i: 164  loss: [[0.13117494]]\n",
      "i: 165  loss: [[0.15376942]]\n",
      "i: 166  loss: [[0.30934394]]\n",
      "i: 167  loss: [[0.21683778]]\n",
      "i: 168  loss: [[0.13116427]]\n",
      "i: 169  loss: [[0.15375852]]\n",
      "i: 170  loss: [[0.30933343]]\n",
      "i: 171  loss: [[0.21682726]]\n",
      "i: 172  loss: [[0.13115362]]\n",
      "i: 173  loss: [[0.15374764]]\n",
      "i: 174  loss: [[0.30932292]]\n",
      "i: 175  loss: [[0.21681675]]\n",
      "i: 176  loss: [[0.13114296]]\n",
      "i: 177  loss: [[0.15373677]]\n",
      "i: 178  loss: [[0.30931242]]\n",
      "i: 179  loss: [[0.21680624]]\n",
      "i: 180  loss: [[0.13113232]]\n",
      "i: 181  loss: [[0.15372591]]\n",
      "i: 182  loss: [[0.30930192]]\n",
      "i: 183  loss: [[0.21679574]]\n",
      "i: 184  loss: [[0.13112168]]\n",
      "i: 185  loss: [[0.15371507]]\n",
      "i: 186  loss: [[0.30929142]]\n",
      "i: 187  loss: [[0.21678523]]\n",
      "i: 188  loss: [[0.13111104]]\n",
      "i: 189  loss: [[0.15370424]]\n",
      "i: 190  loss: [[0.30928092]]\n",
      "i: 191  loss: [[0.21677473]]\n",
      "i: 192  loss: [[0.13110041]]\n",
      "i: 193  loss: [[0.15369342]]\n",
      "i: 194  loss: [[0.30927042]]\n",
      "i: 195  loss: [[0.21676423]]\n",
      "i: 196  loss: [[0.13108978]]\n",
      "i: 197  loss: [[0.15368262]]\n",
      "i: 198  loss: [[0.30925992]]\n",
      "i: 199  loss: [[0.21675373]]\n",
      "i: 200  loss: [[0.13107917]]\n",
      "i: 201  loss: [[0.15367183]]\n",
      "i: 202  loss: [[0.30924943]]\n",
      "i: 203  loss: [[0.21674323]]\n",
      "i: 204  loss: [[0.13106855]]\n",
      "i: 205  loss: [[0.15366105]]\n",
      "i: 206  loss: [[0.30923894]]\n",
      "i: 207  loss: [[0.21673273]]\n",
      "i: 208  loss: [[0.13105794]]\n",
      "i: 209  loss: [[0.15365029]]\n",
      "i: 210  loss: [[0.30922845]]\n",
      "i: 211  loss: [[0.21672224]]\n",
      "i: 212  loss: [[0.13104734]]\n",
      "i: 213  loss: [[0.15363954]]\n",
      "i: 214  loss: [[0.30921796]]\n",
      "i: 215  loss: [[0.21671175]]\n",
      "i: 216  loss: [[0.13103674]]\n",
      "i: 217  loss: [[0.15362881]]\n",
      "i: 218  loss: [[0.30920747]]\n",
      "i: 219  loss: [[0.21670126]]\n",
      "i: 220  loss: [[0.13102615]]\n",
      "i: 221  loss: [[0.15361808]]\n",
      "i: 222  loss: [[0.30919699]]\n",
      "i: 223  loss: [[0.21669077]]\n",
      "i: 224  loss: [[0.13101557]]\n",
      "i: 225  loss: [[0.15360737]]\n",
      "i: 226  loss: [[0.3091865]]\n",
      "i: 227  loss: [[0.21668028]]\n",
      "i: 228  loss: [[0.13100499]]\n",
      "i: 229  loss: [[0.15359668]]\n",
      "i: 230  loss: [[0.30917602]]\n",
      "i: 231  loss: [[0.2166698]]\n",
      "i: 232  loss: [[0.13099441]]\n",
      "i: 233  loss: [[0.153586]]\n",
      "i: 234  loss: [[0.30916554]]\n",
      "i: 235  loss: [[0.21665932]]\n",
      "i: 236  loss: [[0.13098384]]\n",
      "i: 237  loss: [[0.15357533]]\n",
      "i: 238  loss: [[0.30915507]]\n",
      "i: 239  loss: [[0.21664884]]\n",
      "i: 240  loss: [[0.13097328]]\n",
      "i: 241  loss: [[0.15356468]]\n",
      "i: 242  loss: [[0.30914459]]\n",
      "i: 243  loss: [[0.21663836]]\n",
      "i: 244  loss: [[0.13096272]]\n",
      "i: 245  loss: [[0.15355403]]\n",
      "i: 246  loss: [[0.30913412]]\n",
      "i: 247  loss: [[0.21662788]]\n",
      "i: 248  loss: [[0.13095217]]\n",
      "i: 249  loss: [[0.15354341]]\n",
      "i: 250  loss: [[0.30912365]]\n",
      "i: 251  loss: [[0.21661741]]\n",
      "i: 252  loss: [[0.13094162]]\n",
      "i: 253  loss: [[0.15353279]]\n",
      "i: 254  loss: [[0.30911318]]\n",
      "i: 255  loss: [[0.21660694]]\n",
      "i: 256  loss: [[0.13093108]]\n",
      "i: 257  loss: [[0.15352219]]\n",
      "i: 258  loss: [[0.30910271]]\n",
      "i: 259  loss: [[0.21659647]]\n",
      "i: 260  loss: [[0.13092055]]\n",
      "i: 261  loss: [[0.1535116]]\n",
      "i: 262  loss: [[0.30909225]]\n",
      "i: 263  loss: [[0.216586]]\n",
      "i: 264  loss: [[0.13091002]]\n",
      "i: 265  loss: [[0.15350103]]\n",
      "i: 266  loss: [[0.30908179]]\n",
      "i: 267  loss: [[0.21657554]]\n",
      "i: 268  loss: [[0.13089949]]\n",
      "i: 269  loss: [[0.15349046]]\n",
      "i: 270  loss: [[0.30907133]]\n",
      "i: 271  loss: [[0.21656507]]\n",
      "i: 272  loss: [[0.13088897]]\n",
      "i: 273  loss: [[0.15347992]]\n",
      "i: 274  loss: [[0.30906087]]\n",
      "i: 275  loss: [[0.21655461]]\n",
      "i: 276  loss: [[0.13087846]]\n",
      "i: 277  loss: [[0.15346938]]\n",
      "i: 278  loss: [[0.30905041]]\n",
      "i: 279  loss: [[0.21654415]]\n",
      "i: 280  loss: [[0.13086795]]\n",
      "i: 281  loss: [[0.15345886]]\n",
      "i: 282  loss: [[0.30903996]]\n",
      "i: 283  loss: [[0.2165337]]\n",
      "i: 284  loss: [[0.13085745]]\n",
      "i: 285  loss: [[0.15344835]]\n",
      "i: 286  loss: [[0.30902951]]\n",
      "i: 287  loss: [[0.21652324]]\n",
      "i: 288  loss: [[0.13084696]]\n",
      "i: 289  loss: [[0.15343785]]\n",
      "i: 290  loss: [[0.30901906]]\n",
      "i: 291  loss: [[0.21651279]]\n",
      "i: 292  loss: [[0.13083647]]\n",
      "i: 293  loss: [[0.15342737]]\n",
      "i: 294  loss: [[0.30900861]]\n",
      "i: 295  loss: [[0.21650234]]\n",
      "i: 296  loss: [[0.13082598]]\n",
      "i: 297  loss: [[0.1534169]]\n",
      "i: 298  loss: [[0.30899816]]\n",
      "i: 299  loss: [[0.2164919]]\n",
      "i: 300  loss: [[0.1308155]]\n",
      "i: 301  loss: [[0.15340644]]\n",
      "i: 302  loss: [[0.30898772]]\n",
      "i: 303  loss: [[0.21648145]]\n",
      "i: 304  loss: [[0.13080503]]\n",
      "i: 305  loss: [[0.153396]]\n",
      "i: 306  loss: [[0.30897728]]\n",
      "i: 307  loss: [[0.21647101]]\n",
      "i: 308  loss: [[0.13079456]]\n",
      "i: 309  loss: [[0.15338557]]\n",
      "i: 310  loss: [[0.30896684]]\n",
      "i: 311  loss: [[0.21646057]]\n",
      "i: 312  loss: [[0.1307841]]\n",
      "i: 313  loss: [[0.15337515]]\n",
      "i: 314  loss: [[0.3089564]]\n",
      "i: 315  loss: [[0.21645013]]\n",
      "i: 316  loss: [[0.13077364]]\n",
      "i: 317  loss: [[0.15336474]]\n",
      "i: 318  loss: [[0.30894597]]\n",
      "i: 319  loss: [[0.2164397]]\n",
      "i: 320  loss: [[0.13076319]]\n",
      "i: 321  loss: [[0.15335435]]\n",
      "i: 322  loss: [[0.30893554]]\n",
      "i: 323  loss: [[0.21642926]]\n",
      "i: 324  loss: [[0.13075275]]\n",
      "i: 325  loss: [[0.15334397]]\n",
      "i: 326  loss: [[0.30892511]]\n",
      "i: 327  loss: [[0.21641883]]\n",
      "i: 328  loss: [[0.13074231]]\n",
      "i: 329  loss: [[0.1533336]]\n",
      "i: 330  loss: [[0.30891468]]\n",
      "i: 331  loss: [[0.21640841]]\n",
      "i: 332  loss: [[0.13073187]]\n",
      "i: 333  loss: [[0.15332325]]\n",
      "i: 334  loss: [[0.30890426]]\n",
      "i: 335  loss: [[0.21639798]]\n",
      "i: 336  loss: [[0.13072145]]\n",
      "i: 337  loss: [[0.15331291]]\n",
      "i: 338  loss: [[0.30889384]]\n",
      "i: 339  loss: [[0.21638756]]\n",
      "i: 340  loss: [[0.13071102]]\n",
      "i: 341  loss: [[0.15330258]]\n",
      "i: 342  loss: [[0.30888342]]\n",
      "i: 343  loss: [[0.21637714]]\n",
      "i: 344  loss: [[0.13070061]]\n",
      "i: 345  loss: [[0.15329227]]\n",
      "i: 346  loss: [[0.308873]]\n",
      "i: 347  loss: [[0.21636672]]\n",
      "i: 348  loss: [[0.1306902]]\n",
      "i: 349  loss: [[0.15328197]]\n",
      "i: 350  loss: [[0.30886258]]\n",
      "i: 351  loss: [[0.2163563]]\n",
      "i: 352  loss: [[0.13067979]]\n",
      "i: 353  loss: [[0.15327168]]\n",
      "i: 354  loss: [[0.30885217]]\n",
      "i: 355  loss: [[0.21634589]]\n",
      "i: 356  loss: [[0.13066939]]\n",
      "i: 357  loss: [[0.1532614]]\n",
      "i: 358  loss: [[0.30884176]]\n",
      "i: 359  loss: [[0.21633548]]\n",
      "i: 360  loss: [[0.130659]]\n",
      "i: 361  loss: [[0.15325113]]\n",
      "i: 362  loss: [[0.30883135]]\n",
      "i: 363  loss: [[0.21632507]]\n",
      "i: 364  loss: [[0.13064861]]\n",
      "i: 365  loss: [[0.15324088]]\n",
      "i: 366  loss: [[0.30882094]]\n",
      "i: 367  loss: [[0.21631466]]\n",
      "i: 368  loss: [[0.13063823]]\n",
      "i: 369  loss: [[0.15323064]]\n",
      "i: 370  loss: [[0.30881054]]\n",
      "i: 371  loss: [[0.21630426]]\n",
      "i: 372  loss: [[0.13062785]]\n",
      "i: 373  loss: [[0.15322042]]\n",
      "i: 374  loss: [[0.30880014]]\n",
      "i: 375  loss: [[0.21629386]]\n",
      "i: 376  loss: [[0.13061748]]\n",
      "i: 377  loss: [[0.15321021]]\n",
      "i: 378  loss: [[0.30878974]]\n",
      "i: 379  loss: [[0.21628346]]\n",
      "i: 380  loss: [[0.13060711]]\n",
      "i: 381  loss: [[0.1532]]\n",
      "i: 382  loss: [[0.30877935]]\n",
      "i: 383  loss: [[0.21627306]]\n",
      "i: 384  loss: [[0.13059675]]\n",
      "i: 385  loss: [[0.15318982]]\n",
      "i: 386  loss: [[0.30876895]]\n",
      "i: 387  loss: [[0.21626267]]\n",
      "i: 388  loss: [[0.1305864]]\n",
      "i: 389  loss: [[0.15317964]]\n",
      "i: 390  loss: [[0.30875856]]\n",
      "i: 391  loss: [[0.21625228]]\n",
      "i: 392  loss: [[0.13057605]]\n",
      "i: 393  loss: [[0.15316948]]\n",
      "i: 394  loss: [[0.30874817]]\n",
      "i: 395  loss: [[0.21624189]]\n",
      "i: 396  loss: [[0.1305657]]\n",
      "i: 397  loss: [[0.15315933]]\n",
      "i: 398  loss: [[0.30873778]]\n",
      "i: 399  loss: [[0.2162315]]\n",
      "i: 400  loss: [[0.13055537]]\n",
      "i: 401  loss: [[0.15314919]]\n",
      "i: 402  loss: [[0.3087274]]\n",
      "i: 403  loss: [[0.21622112]]\n",
      "i: 404  loss: [[0.13054504]]\n",
      "i: 405  loss: [[0.15313906]]\n",
      "i: 406  loss: [[0.30871702]]\n",
      "i: 407  loss: [[0.21621074]]\n",
      "i: 408  loss: [[0.13053471]]\n",
      "i: 409  loss: [[0.15312895]]\n",
      "i: 410  loss: [[0.30870664]]\n",
      "i: 411  loss: [[0.21620036]]\n",
      "i: 412  loss: [[0.13052439]]\n",
      "i: 413  loss: [[0.15311885]]\n",
      "i: 414  loss: [[0.30869626]]\n",
      "i: 415  loss: [[0.21618998]]\n",
      "i: 416  loss: [[0.13051407]]\n",
      "i: 417  loss: [[0.15310876]]\n",
      "i: 418  loss: [[0.30868589]]\n",
      "i: 419  loss: [[0.21617961]]\n",
      "i: 420  loss: [[0.13050377]]\n",
      "i: 421  loss: [[0.15309868]]\n",
      "i: 422  loss: [[0.30867552]]\n",
      "i: 423  loss: [[0.21616924]]\n",
      "i: 424  loss: [[0.13049346]]\n",
      "i: 425  loss: [[0.15308862]]\n",
      "i: 426  loss: [[0.30866515]]\n",
      "i: 427  loss: [[0.21615887]]\n",
      "i: 428  loss: [[0.13048316]]\n",
      "i: 429  loss: [[0.15307857]]\n",
      "i: 430  loss: [[0.30865478]]\n",
      "i: 431  loss: [[0.21614851]]\n",
      "i: 432  loss: [[0.13047287]]\n",
      "i: 433  loss: [[0.15306853]]\n",
      "i: 434  loss: [[0.30864442]]\n",
      "i: 435  loss: [[0.21613815]]\n",
      "i: 436  loss: [[0.13046259]]\n",
      "i: 437  loss: [[0.1530585]]\n",
      "i: 438  loss: [[0.30863406]]\n",
      "i: 439  loss: [[0.21612779]]\n",
      "i: 440  loss: [[0.1304523]]\n",
      "i: 441  loss: [[0.15304849]]\n",
      "i: 442  loss: [[0.3086237]]\n",
      "i: 443  loss: [[0.21611743]]\n",
      "i: 444  loss: [[0.13044203]]\n",
      "i: 445  loss: [[0.15303849]]\n",
      "i: 446  loss: [[0.30861334]]\n",
      "i: 447  loss: [[0.21610707]]\n",
      "i: 448  loss: [[0.13043176]]\n",
      "i: 449  loss: [[0.1530285]]\n",
      "i: 450  loss: [[0.30860299]]\n",
      "i: 451  loss: [[0.21609672]]\n",
      "i: 452  loss: [[0.1304215]]\n",
      "i: 453  loss: [[0.15301852]]\n",
      "i: 454  loss: [[0.30859264]]\n",
      "i: 455  loss: [[0.21608637]]\n",
      "i: 456  loss: [[0.13041124]]\n",
      "i: 457  loss: [[0.15300856]]\n",
      "i: 458  loss: [[0.30858229]]\n",
      "i: 459  loss: [[0.21607603]]\n",
      "i: 460  loss: [[0.13040098]]\n",
      "i: 461  loss: [[0.1529986]]\n",
      "i: 462  loss: [[0.30857195]]\n",
      "i: 463  loss: [[0.21606568]]\n",
      "i: 464  loss: [[0.13039074]]\n",
      "i: 465  loss: [[0.15298866]]\n",
      "i: 466  loss: [[0.3085616]]\n",
      "i: 467  loss: [[0.21605534]]\n",
      "i: 468  loss: [[0.1303805]]\n",
      "i: 469  loss: [[0.15297873]]\n",
      "i: 470  loss: [[0.30855126]]\n",
      "i: 471  loss: [[0.216045]]\n",
      "i: 472  loss: [[0.13037026]]\n",
      "i: 473  loss: [[0.15296882]]\n",
      "i: 474  loss: [[0.30854093]]\n",
      "i: 475  loss: [[0.21603466]]\n",
      "i: 476  loss: [[0.13036003]]\n",
      "i: 477  loss: [[0.15295891]]\n",
      "i: 478  loss: [[0.30853059]]\n",
      "i: 479  loss: [[0.21602433]]\n",
      "i: 480  loss: [[0.13034981]]\n",
      "i: 481  loss: [[0.15294902]]\n",
      "i: 482  loss: [[0.30852026]]\n",
      "i: 483  loss: [[0.216014]]\n",
      "i: 484  loss: [[0.13033959]]\n",
      "i: 485  loss: [[0.15293914]]\n",
      "i: 486  loss: [[0.30850993]]\n",
      "i: 487  loss: [[0.21600367]]\n",
      "i: 488  loss: [[0.13032937]]\n",
      "i: 489  loss: [[0.15292927]]\n",
      "i: 490  loss: [[0.3084996]]\n",
      "i: 491  loss: [[0.21599335]]\n",
      "i: 492  loss: [[0.13031917]]\n",
      "i: 493  loss: [[0.15291941]]\n",
      "i: 494  loss: [[0.30848927]]\n",
      "i: 495  loss: [[0.21598303]]\n",
      "i: 496  loss: [[0.13030896]]\n",
      "i: 497  loss: [[0.15290957]]\n",
      "i: 498  loss: [[0.30847895]]\n",
      "i: 499  loss: [[0.21597271]]\n"
     ]
    }
   ],
   "source": [
    "nr_split = 3\n",
    "kfold_Out = KFold(nr_split,FixedInputData)\n",
    "kfold_Out[0].X_Train.shape[0]\n",
    "\n",
    "\n",
    "learningRate = 0.000001\n",
    "nr_epoch = 500\n",
    "\n",
    "for i in range (1): #TODO\n",
    "    # for each split do the training\n",
    "    X = kfold_Out[i].X_Train\n",
    "    Y = np.reshape( kfold_Out[i].Y_Train,(1, kfold_Out[i].Y_Train.shape[0]))\n",
    "    M = X.shape[0]\n",
    "    W = np.zeros((1 ,X.shape[1])) # 1 * 72\n",
    "    b = np.zeros((1,1)) # 1 * 1\n",
    "\n",
    "    for j in range(nr_epoch): # traning for nr_epoch\n",
    "            \n",
    "        Z = CalculateYHat(W,X,b)  # W => 1 * 72 ||||| X => M * 72 \n",
    "        \n",
    "        A = sigmoid(Z) # Y_hat\n",
    "        \n",
    "        L = CrossEntropyLossFunction(M,A, Y)\n",
    "        \n",
    "        # dL/dZ\n",
    "        dZ = A - Y \n",
    "        print(\"i:\" , j , \" loss:\",L)\n",
    "        #print(Z)\n",
    "        # Calculate derivitives\n",
    "        dW = (1/M) * (np.dot(X.T,dZ.T)) # dL/dW\n",
    "        \n",
    "        db = (1/M) * np.sum(dZ) # dL/db\n",
    "\n",
    "        # update the weights and bias\n",
    "        W = W - learningRate * np.transpose(dW)\n",
    "        \n",
    "        b = b - learningRate * np.transpose(db)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    # Calculate loss for current split\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "51902d065fd6e660be0379b1baf3c7c6b11efc2d50f5e5f8e7ba9613741f3e85"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
